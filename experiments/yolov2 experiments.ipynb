{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Input, MaxPooling2D, BatchNormalization, Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import numpy as np\n",
    "from utils import read_labels, draw_image, image_to_yolo_input, Object, parse_annotation, Annotation, calculate_IoU, image_to_vgg_input, image_to_mobilenet_input, LabelEncoder\n",
    "from augmentation import read_image, display_image, change_brightness_slightly, change_brightness_not_so_slightly, dropout, adjust_contrast, grayscale, noise, blur, sharpen\n",
    "from PIL import Image as Img\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import uuid\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: read from config\n",
    "\n",
    "image_width = 416\n",
    "image_height = 416\n",
    "grid_width = int(image_width / 32) # 13\n",
    "grid_height = int(image_height / 32) # 13\n",
    "\n",
    "cell_width = image_width / grid_width\n",
    "cell_height = image_height / grid_height\n",
    "\n",
    "boxes = 5\n",
    "\n",
    "activation_alpha = 0.1\n",
    "\n",
    "object_scale = 20\n",
    "noobject_scale = 1\n",
    "class_scale = 4\n",
    "coord_scale = 4\n",
    "\n",
    "threshhold = 0.5\n",
    "nms_threshhold = 0.5#0.5\n",
    "\n",
    "anchors = np.array([[1.05, 1.65], [2.44, 4.13], [4.01, 8.46], [7.62, 5.13], [9.97, 10.43]], dtype = np.float32) # obtained from KMeans experiments ipynb\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "labels_dir = \"./labels.txt\"\n",
    "\n",
    "annotation_folder = '.\\VOCdevkit\\VOC2007\\Annotations'\n",
    "images_folder = '.\\VOCdevkit\\VOC2007\\JPEGImages'\n",
    "\n",
    "murka = r'.\\mytestimages\\murka.jpg'\n",
    "\n",
    "test_annotation = r'.\\VOCdevkit\\VOC2007\\Annotations\\000113.xml'\n",
    "test_image = r'.\\VOCdevkit\\VOC2007\\JPEGImages\\000113.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['horse', 'diningtable', 'sofa', 'train', 'bird', 'aeroplane', 'person', 'boat', 'bottle', 'motorbike', 'bus', 'cat', 'pottedplant', 'car', 'dog', 'bicycle', 'sheep', 'cow', 'tvmonitor', 'chair']\n"
     ]
    }
   ],
   "source": [
    "labels, labels_count = read_labels(labels_dir)\n",
    "\n",
    "encoder = LabelEncoder(labels)\n",
    "\n",
    "print(labels_count)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_tinyyolov2():\n",
    "    layers = []\n",
    "\n",
    "    layers.append(Input(shape=(image_width, image_height, 3)))\n",
    "\n",
    "    layers.append(Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_1\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_1\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_1\", alpha = activation_alpha))\n",
    "    layers.append(MaxPooling2D(name = \"maxpool_1\"))\n",
    "\n",
    "    layers.append(Conv2D(filters = 32, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_2\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_2\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_2\", alpha = activation_alpha))\n",
    "    layers.append(MaxPooling2D(name = \"maxpool_2\"))\n",
    "\n",
    "    layers.append(Conv2D(filters = 64, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_3\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_3\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_3\", alpha = activation_alpha))\n",
    "    layers.append(MaxPooling2D(name = \"maxpool_3\"))\n",
    "\n",
    "    layers.append(Conv2D(filters = 128, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_4\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_4\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_4\", alpha = activation_alpha))\n",
    "    layers.append(MaxPooling2D(name = \"maxpool_4\"))\n",
    "\n",
    "    layers.append(Conv2D(filters = 256, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_5\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_5\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_5\", alpha = activation_alpha))\n",
    "    layers.append(MaxPooling2D(name = \"maxpool_5\"))\n",
    "\n",
    "    layers.append(Conv2D(filters = 512, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_6\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_6\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_6\", alpha = activation_alpha))\n",
    "    #layers.append(MaxPooling2D(name = \"maxpool_6\", pool_size = (2, 2), strides = (1, 1)))\n",
    "\n",
    "    layers.append(Conv2D(filters = 1024, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_7\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_7\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_7\", alpha = activation_alpha))\n",
    "\n",
    "    layers.append(Conv2D(filters = 1024, kernel_size = (3, 3), padding = \"same\", use_bias = False, name=\"conv_8\"))\n",
    "    layers.append(BatchNormalization(name = \"norm_8\"))\n",
    "    layers.append(LeakyReLU(name = \"leaky_8\", alpha = activation_alpha))\n",
    "\n",
    "    layers.append(Conv2D(filters = (boxes * (4 + 1 + labels_count)), kernel_size = (1, 1), padding = \"same\", name=\"conv_9\"))\n",
    "\n",
    "    layers.append(Reshape(target_shape = (grid_width, grid_height, boxes, 5 + labels_count), name = \"output\"))\n",
    "\n",
    "    tinyyolov2 = Sequential(layers = layers, name = \"tiny yolov2 voc\")\n",
    "    tinyyolov2.summary()\n",
    "    \n",
    "    return tinyyolov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_annotations_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1228 00:46:59.509115 17396 deprecation.py:323] From <ipython-input-8-5e31b72149f0>:15: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "assume_batch_size = 10\n",
    "assume_grid_width = 3\n",
    "assume_grid_height = 3\n",
    "assume_boxes = 2\n",
    "\n",
    "cell_x = np.reshape(np.repeat(np.tile(range(assume_grid_width), assume_batch_size * assume_grid_height), assume_boxes), (assume_batch_size, assume_grid_width, assume_grid_height, assume_boxes))\n",
    "cell_y = np.transpose(cell_x, (0,2,1,3))\n",
    "\n",
    "#print(cell_y.shape)\n",
    "#print(cell_y)\n",
    "#print('=====================================')\n",
    "#print(cell_index)\n",
    "\n",
    "cell_x = tf.to_float(tf.reshape(tf.keras.backend.repeat_elements(tf.tile(tf.range(assume_grid_width), [assume_batch_size * assume_grid_height]), assume_boxes, axis=0), \n",
    "                        (assume_batch_size, assume_grid_width, assume_grid_height, assume_boxes)))\n",
    "cell_y = tf.transpose(cell_x, (0,2,1,3))\n",
    "\n",
    "#print(cell_x)\n",
    "#print('=====================================')\n",
    "#print(cell_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef custom_loss(y_true, y_pred):\\nc_pred = tf.sigmoid(y_pred[:, :, :, :, 0])\\nc_true = y_true[:, :, :, :, 0]\\n\\ngreaters = tf.greater(c_true, 0.0)\\n\\nmask_shape = (batch_size, grid_width, grid_height, boxes)\\nobjs = tf.ones(shape = (mask_shape)) * object_scale\\nnoobjs = tf.ones(shape = (mask_shape)) * noobject_scale\\n\\ncoef = tf.where(greaters, objs, noobjs) \\n\\n\\n\\nreturn coef * (c_true - c_pred) ** 2\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    intermins = tf.maximum(xy_true, xy_pred)\n",
    "    intermaxes = tf.minimum(xy_true, xy_pred)\n",
    "\n",
    "    interArea = tf.maximum(0.0, intermaxes[..., 0] - intermins[..., 0] + 1) * tf.maximum(0.0, intermaxes[..., 1] - intermins[..., 1] + 1)\n",
    "\n",
    "    groundTruthArea = (wh_true[..., 0] + 1) * (wh_true[..., 1] + 1)\n",
    "    predictedArea = (wh_pred[..., 0] + 1) * (wh_pred[..., 1] + 1)\n",
    "\n",
    "    iou = interArea / (groundTruthArea + predictedArea - interArea)    \n",
    "    \n",
    "    xpred = y_pred[:, :, :, :, 1]   \n",
    "    ypred = y_pred[:, :, :, :, 2]                \n",
    "    xtrue = y_true[:, :, :, :, 1]\n",
    "    ytrue = y_true[:, :, :, :, 2]\n",
    "\n",
    "    wpred = y_pred[:, :, :, :, 3]\n",
    "    hpred = y_pred[:, :, :, :, 4]                \n",
    "    wtrue = y_true[:, :, :, :, 3]\n",
    "    htrue = y_true[:, :, :, :, 4]\n",
    "\n",
    "    xy_pred = y_pred[:, :, :, :, 1 : 3]\n",
    "    xy_true = y_true[:, :, :, :, 1 : 3]\n",
    "\n",
    "    wh_pred = y_pred[:, :, :, :, 3 : 5]\n",
    "    wh_true = y_true[:, :, :, :, 3 : 5]\n",
    "    '''   \n",
    "    \n",
    "    \n",
    "    #loss = tf.reduce_mean(loss)\n",
    "    #print(np.any(np.isnan(xywhcoef)))\n",
    "    #print(np.any(np.isnan(((xtrue - xpred) ** 2))))\n",
    "    #print(np.any(np.isnan(xywhcoef * ((xtrue - xpred) ** 2))))\n",
    "    #print(loss)\n",
    "    \n",
    "\n",
    "'''\n",
    "def custom_loss(y_true, y_pred):\n",
    "    c_pred = tf.sigmoid(y_pred[:, :, :, :, 0])\n",
    "    c_true = y_true[:, :, :, :, 0]\n",
    "    \n",
    "    greaters = tf.greater(c_true, 0.0)\n",
    "    \n",
    "    mask_shape = (batch_size, grid_width, grid_height, boxes)\n",
    "    objs = tf.ones(shape = (mask_shape)) * object_scale\n",
    "    noobjs = tf.ones(shape = (mask_shape)) * noobject_scale\n",
    "    \n",
    "    coef = tf.where(greaters, objs, noobjs) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return coef * (c_true - c_pred) ** 2\n",
    "'''\n",
    "\n",
    "\n",
    "#runningman = images[52]\n",
    "#runningmanannot = annotations[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.array(anchors)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (almost) complete yolov2 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode, nms, feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lens: 5011 5011\n",
      "Prepared ins & outs paths\n"
     ]
    }
   ],
   "source": [
    "annotations_files, images = get_annotations_images(annotation_folder, images_folder)\n",
    "#ins = np.array([image_to_mobilenet_input(image, inputshape = (image_width, image_height)) for image in images], dtype=np.float32)\n",
    "#outs = np.array([encode_y_true_from_annotatoin(annotation) for annotation in annotations], dtype=np.float32)\n",
    "\n",
    "#print(ins.shape)\n",
    "#print(outs.shape)\n",
    "print(f'Lens: {len(annotations_files)} {len(images)}')\n",
    "print('Prepared ins & outs paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared annotations in 1.05s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "annotations = [parse_annotation(annotation) for annotation in annotations_files]\n",
    "end = time.time()\n",
    "\n",
    "print(f'Prepared annotations in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(annotations, images, batch_size, raw_files = True):\n",
    "    ins = []\n",
    "    outs = []\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        for index in range(len(images)):\n",
    "            ins.append(image_to_mobilenet_input(images[index], inputshape = (image_width, image_height)))\n",
    "            outs.append(encode_y_true_from_annotation(annotations[index], raw_files))\n",
    "            \n",
    "            if len(ins) == batch_size:\n",
    "                yield (np.array(ins, dtype=np.float32), np.array(outs, dtype=np.float32))\n",
    "                ins = []\n",
    "                outs = []\n",
    "\n",
    "def normalize_image_to_mobilenet_input(im):\n",
    "    im /= 255\n",
    "    im -= 0.5\n",
    "    im *= 2.\n",
    "\n",
    "    return im\n",
    "                \n",
    "def batch_generator_augmentation(annotations, images, batch_size, normalize_function, augmention_functions, aug_chance = 0.5, max_augs = 2, raw_files = True):\n",
    "    ins = []\n",
    "    outs = []\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        for index in range(len(images)):\n",
    "            out = read_image(images[index], (image_width, image_height))\n",
    "            \n",
    "            if random.random() < aug_chance:\n",
    "                \n",
    "                augs = int(random.uniform(0, max_augs)) + 1\n",
    "                for aug in random.choices(augmention_functions, k = augs):\n",
    "                    out = aug(out)\n",
    "            \n",
    "        \n",
    "            ins.append(out)\n",
    "            outs.append(encode_y_true_from_annotation(annotations[index], raw_files))\n",
    "            \n",
    "            if len(ins) == batch_size:\n",
    "                yield (np.array(ins, dtype=np.float32), np.array(outs, dtype=np.float32))\n",
    "                ins = []\n",
    "                outs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert not np.any(np.isnan(ins))\n",
    "#assert not np.any(np.isnan(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch generated in 1.57s\n"
     ]
    }
   ],
   "source": [
    "generator_preprocessed = batch_generator(annotations, images, batch_size, raw_files=False)\n",
    "\n",
    "start = time.time()\n",
    "next(generator_preprocessed)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Batch generated in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch generated in 1.12s\n"
     ]
    }
   ],
   "source": [
    "generator_raw = batch_generator(annotations_files, images, batch_size, raw_files=True)\n",
    "\n",
    "start = time.time()\n",
    "next(generator_raw)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Batch generated in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's not a significant enough time difference for pre-loading of annotations to be worth it.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''There's not a significant enough time difference for pre-loading of annotations to be worth it.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch generated in 2.26s\n"
     ]
    }
   ],
   "source": [
    "augmenters = [blur, sharpen, noise, adjust_contrast, change_brightness_not_so_slightly, change_brightness_slightly, dropout, grayscale]\n",
    "generator_raw = batch_generator_augmentation(annotations_files, images, batch_size, normalize_image_to_mobilenet_input, augmenters)\n",
    "\n",
    "start = time.time()\n",
    "next(generator_raw)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Batch generated in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 new images in 55.22s\n"
     ]
    }
   ],
   "source": [
    "augmenters = [blur, adjust_contrast, change_brightness_not_so_slightly, change_brightness_slightly, dropout]\n",
    "people_ann_folder = r\"C:\\Users\\Gencho\\Desktop\\ObjectDetection\\experiments\\annotations\"\n",
    "people_im_folder = r\"C:\\Users\\Gencho\\Desktop\\ObjectDetection\\experiments\\images\"\n",
    "testa, testb = get_annotations_images(people_ann_folder, people_im_folder)\n",
    "available_count = len(testa)\n",
    "generate_count = 1000\n",
    "\n",
    "start = time.time()\n",
    "augment_images(people_im_folder, people_ann_folder, augmenters, target_count = available_count + generate_count)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Generated {generate_count} new images in {(end - start):.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image prepared in: 0.00700s\n",
      "Image prepared in: 0.00700s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"There's not a significant time difference when using opencv.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_to_mobilenet_input_opencv(path, inputshape):\n",
    "    im = cv2.imread(path)\n",
    "    im = cv2.resize(im, inputshape)\n",
    "    \n",
    "    im = np.array(im, np.float32)\n",
    "    im = im[..., ::-1]\n",
    "    im /= 255\n",
    "    im -= 0.5\n",
    "    im *= 2.\n",
    "    \n",
    "    return im\n",
    "\n",
    "start = time.time()\n",
    "a = image_to_mobilenet_input_opencv(test_image, inputshape = (image_width, image_height))\n",
    "end = time.time()\n",
    "\n",
    "print(f'Image prepared in: {(end - start):.5f}s')\n",
    "\n",
    "start = time.time()\n",
    "b = image_to_mobilenet_input(test_image, inputshape = (image_width, image_height))\n",
    "end = time.time()\n",
    "\n",
    "print(f'Image prepared in: {(end - start):.5f}s')\n",
    "'''There's not a significant time difference when using opencv.'''\n",
    "#assert np.array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(a[0])\n",
    "#print('==================')\n",
    "#print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getmobilenetyolov2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled\n"
     ]
    }
   ],
   "source": [
    "mobilenetyolov2 = get_mobilenetyolov2()\n",
    "\n",
    "adam = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "mobilenetyolov2.compile(optimizer = adam, loss = custom_loss)\n",
    "\n",
    "print('Model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testbatchins = ins[:batch_size]\n",
    "#testbatchouts = outs[:batch_size]\n",
    "#testbatchins = np.array([image_to_mobilenet_input(test_image, (image_width, image_height))], dtype=np.float32)\n",
    "#testbatchouts = np.array([encode_y_true_from_annotatoin(test_annotation)], dtype=np.float32)\n",
    "    \n",
    "#pred = mobilenetyolov2.predict(testbatchins)\n",
    "#loss = custom_loss(testbatchouts, pred)\n",
    "\n",
    "#assert not np.any(np.isnan(pred))\n",
    "#assert not np.any(np.isnan(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobilenetyolov2.load_weights('./weights/mobilenetyolov2try07abitofaugmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "epochs = 250\n",
    "augmenters = [blur, sharpen, noise, adjust_contrast, change_brightness_not_so_slightly, change_brightness_slightly, dropout, grayscale]\n",
    "generator_raw = batch_generator_augmentation(annotations_files, images, batch_size, normalize_image_to_mobilenet_input, augmenters)\n",
    "\n",
    "\n",
    "h = mobilenetyolov2.fit_generator(generator_raw, steps_per_epoch = len(images) // batch_size, epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mobilenetyolov2.save_weights('./weights/mobilenetyolov2try07abitofaugmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 479-480: truncated \\UXXXXXXXX escape (<ipython-input-43-a46673e52271>, line 24)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-a46673e52271>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 479-480: truncated \\UXXXXXXXX escape\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "'''\n",
    "def filter_people(annotations, images):\n",
    "    people_ann = []\n",
    "    people_im = []\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        image, annotation = images[i], annotations[i]\n",
    "        if any(obj.name == \"person\" for obj in parse_annotation(annotation).objects):\n",
    "            people_ann.append(annotation)\n",
    "            people_im.append(image)\n",
    "                \n",
    "    return people_ann, people_im\n",
    "\n",
    "people_ann, people_im = filter_people(annotations_files, images)\n",
    "\n",
    "\n",
    "\n",
    "ann_target_folder = r\"C:\\Users\\Gencho\\Desktop\\ObjectDetection\\experiments\\annotations\"\n",
    "im_target_folder = r\"C:\\Users\\Gencho\\Desktop\\ObjectDetection\\experiments\\images\"\n",
    "\n",
    "for i in range(len(people_ann)):\n",
    "    shutil.copy(people_ann[i], f'{ann_target_folder}\\\\{i}.xml')\n",
    "    shutil.copy(people_im[i], f'{im_target_folder}\\\\{i}.jpg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threshhold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshhold = 0.2\n",
    "objs = feed_forward(mobilenetyolov2, images[index], True)\n",
    "for obj in objs:\n",
    "    print(obj)\n",
    "objects_trough_nms = group_nms(objs)\n",
    "draw_image(images[index], objects_trough_nms, draw_grid = True, grid_size = (grid_width, grid_height))\n",
    "for obj in objects_trough_nms:\n",
    "    print(obj)\n",
    "threshhold = 0.5\n",
    "index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#index = 52\n",
    "#runningman = images[52]\n",
    "'''\n",
    "encoded = encode_y_true_from_annotatoin(annotations[index])\n",
    "for row in range(grid_width):\n",
    "    for col in range(grid_height):\n",
    "        for box in range(boxes):\n",
    "            if encoded[row, col, box, 0]== 1:\n",
    "                print(f'{row} {col} {box}')\n",
    "'''\n",
    "\n",
    "threshhold = 0.1\n",
    "objs = feed_forward(mobilenetyolov2, murka, False)\n",
    "objs = nms(objs)\n",
    "for obj in objs:\n",
    "    print(obj)\n",
    "draw_image(murka, objs)\n",
    "threshhold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "things to consider:\n",
    "Lambda layer which decodes output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "    possible consideration:\n",
    "        if the network makes a prediction with an IoU > 0.6 in a detector which was not chosen for the object do not penalise it\n",
    "      \n",
    "    train on entire voc 2012 + 2007 not only 2007\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sanity check if images are preprocessed correctly for mobilenet'''\n",
    "#mobilenet = MobileNet(weights = 'imagenet')\n",
    "\n",
    "#from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "\n",
    "#test = np.array([image_to_mobilenet_input(r'.\\VOCdevkit\\VOC2007\\JPEGImages\\000019.jpg', inputshape=(224, 224))])\n",
    "#res = mobilenet.predict(test)\n",
    "#print(decode_predictions(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}